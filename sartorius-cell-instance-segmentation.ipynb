{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport glob\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img\nimport os\nimport collections\nimport random\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport pickle\nimport json\nimport rasterio\nfrom matplotlib.path import Path\n\n\nfrom PIL import Image\nfrom PIL import ImageFilter\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision.transforms import ToPILImage\nfrom torchvision.transforms import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data import ConcatDataset\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\nfrom torchvision.transforms.functional import InterpolationMode\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n%matplotlib inline\n","metadata":{"execution":{"iopub.status.busy":"2022-02-05T19:30:52.698036Z","iopub.execute_input":"2022-02-05T19:30:52.698384Z","iopub.status.idle":"2022-02-05T19:30:56.852359Z","shell.execute_reply.started":"2022-02-05T19:30:52.698290Z","shell.execute_reply":"2022-02-05T19:30:56.851659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torchsummary\nfrom torchsummary import summary","metadata":{"execution":{"iopub.status.busy":"2022-02-05T19:30:56.853961Z","iopub.execute_input":"2022-02-05T19:30:56.854165Z","iopub.status.idle":"2022-02-05T19:31:06.983124Z","shell.execute_reply.started":"2022-02-05T19:30:56.854140Z","shell.execute_reply":"2022-02-05T19:31:06.982267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ../input/segmentation-model-wheels/efficientnet_pytorch-0.6.3-py3-none-any.whl\n!pip install ../input/segmentation-model-wheels/pretrainedmodels-0.7.4-py3-none-any.whl\n!pip install ../input/segmentation-model-wheels/timm-0.4.12-py3-none-any.whl\n!pip install ../input/segmentation-model-wheels/segmentation_models_pytorch-0.2.1-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2022-02-05T19:31:06.984630Z","iopub.execute_input":"2022-02-05T19:31:06.984856Z","iopub.status.idle":"2022-02-05T19:31:40.435351Z","shell.execute_reply.started":"2022-02-05T19:31:06.984829Z","shell.execute_reply":"2022-02-05T19:31:40.434671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import segmentation_models_pytorch as smp","metadata":{"execution":{"iopub.status.busy":"2022-02-05T19:31:40.436808Z","iopub.execute_input":"2022-02-05T19:31:40.437126Z","iopub.status.idle":"2022-02-05T19:31:46.476603Z","shell.execute_reply.started":"2022-02-05T19:31:40.437083Z","shell.execute_reply":"2022-02-05T19:31:46.475858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DEFINE GLOBAL CONSTANTS","metadata":{}},{"cell_type":"code","source":"#Define constants\nTRAIN_IMAGE_PATH = '../input/sartorius-cell-instance-segmentation/train'\nTRAIN_LABEL_PATH = '../input/sartorius-cell-instance-segmentation/train.csv'\nTEST_IMAGE_PATH = '../input/sartorius-cell-instance-segmentation/test'\n\nWIDTH = 704\nHEIGHT = 520\n\nTEST = True\nBEST_EPOCH = 26 #Set to None if not known\n\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(\"Device: \", DEVICE)\n\nRESNET_MEAN = (0.485, 0.456, 0.406) #change to mean of training images\nRESNET_STD = (0.229, 0.224, 0.225) #change to std dev of training images\n#RESNET_MEAN = (0.5, )\n#RESNET_STD = (0.5, )\n\nBATCH_SIZE = 2\n\n# No changes tried with the optimizer yet.\nMOMENTUM = 0.9\nLEARNING_RATE = 0.001\nWEIGHT_DECAY = 0.0005\n\n# Changes the confidence required for a pixel to be kept for a mask. \n# Only used 0.5 till now.\nMASK_THRESHOLD = 0.5\n\n# Normalize to resnet mean and std if True.\nNORMALIZE = True \n\nresize_factor = False # 0.5\n\n# Use a StepLR scheduler if True. Not tried yet.\nUSE_SCHEDULER = False\n\n# Amount of epochs\nNUM_EPOCHS = 30\n\nMIN_SCORE = 0.59\n\ncell_type_dict = {\"astro\": 1, \"cort\": 2, \"shsy5y\": 3}\n\nmask_threshold_dict = {1: 0.55, 2: 0.75, 3:  0.6}\nmin_score_dict = {1: 0.55, 2: 0.75, 3: 0.5}\n\nPCT_IMAGES_VALIDATION = 0.075\n\nBOX_DETECTIONS_PER_IMG = 540\n\nUSE_LIVECELL = False\n\nUSE_MASK_RCNN = True\n\nUSE_UNET_PLUS_PLUS = False","metadata":{"execution":{"iopub.status.busy":"2022-02-05T19:31:46.479292Z","iopub.execute_input":"2022-02-05T19:31:46.480163Z","iopub.status.idle":"2022-02-05T19:31:46.492572Z","shell.execute_reply.started":"2022-02-05T19:31:46.480116Z","shell.execute_reply":"2022-02-05T19:31:46.491622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utilities","metadata":{}},{"cell_type":"code","source":"#Function to decode run length encoding\ndef rle_decode(mask_rle, shape=(520, 704)):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape)  # Needed to align to RLE direction\n\ndef remove_overlapping_pixels(mask, other_masks):\n    for other_mask in other_masks:\n        if np.sum(np.logical_and(mask, other_mask)) > 0:\n            mask[np.logical_and(mask, other_mask)] = 0\n    return mask\n\ndef combine_masks(masks, mask_threshold):\n    \"\"\"\n    combine masks into one image\n    \"\"\"\n    maskimg = np.zeros((HEIGHT, WIDTH))\n    # print(len(masks.shape), masks.shape)\n    for m, mask in enumerate(masks,1):\n        maskimg[mask>mask_threshold] = m + 1\n    return maskimg\n\ndef get_box(a_mask):\n    ''' Get the bounding box of a given mask '''\n    pos = np.where(a_mask)\n    if (len(pos[0]) > 0) and (len(pos[1]) > 0):\n        xmin = np.min(pos[1])\n        xmax = np.max(pos[1])\n        ymin = np.min(pos[0])\n        ymax = np.max(pos[0])\n        \n        if (xmax > xmin) and (ymax > ymin):\n            return [xmin, ymin, xmax, ymax]\n        else:\n            return []\n    else:\n        return []\n            \n    \n\n\ndef rle_encode(x):\n    dots = np.where(x.flatten() == 1)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b>prev+1): run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return ' '.join(map(str, run_lengths))\n\n\ndef get_filtered_masks(pred):\n    \"\"\"\n    filter masks using MIN_SCORE for mask and MAX_THRESHOLD for pixels\n    \"\"\"\n    use_masks = []   \n    for i, mask in enumerate(pred[\"masks\"]):\n\n        # Filter-out low-scoring results. Not tried yet.\n        scr = pred[\"scores\"][i].cpu().item()\n        label = pred[\"labels\"][i].cpu().item()\n        if scr > min_score_dict[label]:\n            mask = mask.cpu().numpy().squeeze()\n            # Keep only highly likely pixels\n            binary_mask = mask > mask_threshold_dict[label]\n            binary_mask = remove_overlapping_pixels(binary_mask, use_masks)\n            use_masks.append(binary_mask)\n\n    return use_masks\n\n\ndef compute_iou(labels, y_pred, verbose=0):\n    \"\"\"\n    Computes the IoU for instance labels and predictions.\n\n    Args:\n        labels (np array): Labels.\n        y_pred (np array): predictions\n\n    Returns:\n        np array: IoU matrix, of size true_objects x pred_objects.\n    \"\"\"\n\n    true_objects = len(np.unique(labels))\n    pred_objects = len(np.unique(y_pred))\n\n    if verbose:\n        print(\"Number of true objects: {}\".format(true_objects))\n        print(\"Number of predicted objects: {}\".format(pred_objects))\n\n    # Compute intersection between all objects\n    intersection = np.histogram2d(\n        labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects)\n    )[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins=true_objects)[0]\n    area_pred = np.histogram(y_pred, bins=pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n    intersection = intersection[1:, 1:] # exclude background\n    union = union[1:, 1:]\n    union[union == 0] = 1e-9\n    iou = intersection / union\n    \n    return iou  \n\ndef precision_at(threshold, iou):\n    \"\"\"\n    Computes the precision at a given threshold.\n\n    Args:\n        threshold (float): Threshold.\n        iou (np array): IoU matrix.\n\n    Returns:\n        int: Number of true positives,\n        int: Number of false positives,\n        int: Number of false negatives.\n    \"\"\"\n    matches = iou > threshold\n    true_positives = np.sum(matches, axis=1) == 1  # Correct objects\n    false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n    false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n    tp, fp, fn = (\n        np.sum(true_positives),\n        np.sum(false_positives),\n        np.sum(false_negatives),\n    )\n    return tp, fp, fn\n\ndef iou_map(truths, preds, verbose=0):\n    \"\"\"\n    Computes the metric for the competition.\n    Masks contain the segmented pixels where each object has one value associated,\n    and 0 is the background.\n\n    Args:\n        truths (list of masks): Ground truths.\n        preds (list of masks): Predictions.\n        verbose (int, optional): Whether to print infos. Defaults to 0.\n\n    Returns:\n        float: mAP.\n    \"\"\"\n    ious = [compute_iou(truth, pred, verbose) for truth, pred in zip(truths, preds)]\n\n    if verbose:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n\n    prec = []\n    for t in np.arange(0.5, 1.0, 0.05):\n        tps, fps, fns = 0, 0, 0\n        for iou in ious:\n            tp, fp, fn = precision_at(t, iou)\n            tps += tp\n            fps += fp\n            fns += fn\n\n        p = tps / (tps + fps + fns)\n        prec.append(p)\n\n        if verbose:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tps, fps, fns, p))\n\n    if verbose:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n\n    return np.mean(prec)\n\n\ndef get_score(ds, mdl):\n    \"\"\"\n    Get average IOU mAP score for a dataset\n    \"\"\"\n    mdl.eval()\n    iouscore = 0\n    iouscore_list = []\n    for i in tqdm(range(len(ds))):\n        img, targets = ds[i]\n        with torch.no_grad():\n            result = mdl([img.to(DEVICE)])[0]\n            \n        masks = combine_masks(targets['masks'], 0.5)\n        labels = pd.Series(result['labels'].cpu().numpy()).value_counts()\n\n        mask_threshold = mask_threshold_dict[labels.sort_values().index[-1]]\n        pred_masks = combine_masks(get_filtered_masks(result), mask_threshold)\n        iouscore += iou_map([masks],[pred_masks])\n        iouscore_list.append(iou_map([masks],[pred_masks]))\n    return iouscore / len(ds), iouscore_list\n\n\ndef get_key(my_dict, val):\n    for key, value in my_dict.items():\n         if val == value:\n                return key\n            \n            \n\ndef convert_livecell_annot_to_dict(livecell_dataset_annot):\n    ids = list()\n    for i,img_dict in enumerate(livecell_dataset_annot[\"images\"]):\n        ids.append(livecell_dataset_annot[\"images\"][i][\"id\"])\n\n    d = {k: {\"segmentation\": [],\"bbox\": [], \"path\": []} for k in ids}\n    for i in range(len(d)):\n        d[livecell_dataset_annot[\"images\"][i][\"id\"]][\"path\"].append(livecell_dataset_annot[\"images\"][i][\"original_filename\"])\n\n    for key in livecell_dataset_annot[\"annotations\"].keys():\n        id = livecell_dataset_annot[\"annotations\"][key][\"image_id\"]\n        seg = livecell_dataset_annot[\"annotations\"][key][\"segmentation\"][0]\n        bbox = livecell_dataset_annot[\"annotations\"][key][\"bbox\"]\n\n        d[id][\"segmentation\"].append(seg)    \n        d[id][\"bbox\"].append(bbox)\n    \n    return d\n\ndef convert_livecell_mask_to_rle(livecell_dataset_segmentation):\n    seg_list = list()\n     \n    for img_mask in livecell_dataset_segmentation:\n\n        x = img_mask[0::2]\n        y = img_mask[1::2]\n\n        arr = [(x, y) for (x, y) in zip(y,x)]\n        vertices = np.asarray(arr)\n        path = Path(vertices)\n        xmin, ymin, xmax, ymax = np.asarray(path.get_extents(), dtype=int).ravel()\n        x, y = np.mgrid[:520, :704]\n\n        # mesh grid to a list of points\n        points = np.vstack((x.ravel(), y.ravel())).T\n\n        # select points included in the path\n        mask = path.contains_points(points)\n        path_points = points[np.where(mask)]\n\n        # reshape mask for display\n        img_mask = mask.reshape(x.shape)\n        img_mask = img_mask.astype(np.int)\n        # ENCODED MASK\n        encoded_img_mask = rle_encode(img_mask)\n        seg_list.append(encoded_img_mask)\n        \n    return seg_list\n","metadata":{"execution":{"iopub.status.busy":"2022-02-05T19:31:46.493995Z","iopub.execute_input":"2022-02-05T19:31:46.494197Z","iopub.status.idle":"2022-02-05T19:31:46.541311Z","shell.execute_reply.started":"2022-02-05T19:31:46.494173Z","shell.execute_reply":"2022-02-05T19:31:46.540455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<iframe src=\"https://www.kaggle.com/embed/rluethy/sartorius-torch-mask-r-cnn/notebook?cellIds=10&kernelSessionId=78966534\" height=\"300\" style=\"margin: 0 auto; width: 100%; max-width: 950px;\" frameborder=\"0\" scrolling=\"auto\" title=\"🦠 Sartorius - Torch Mask R-CNN\"></iframe>","metadata":{}},{"cell_type":"code","source":"# Fix randomness\ndef fix_all_seeds(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    \nfix_all_seeds(2021)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T19:31:46.542689Z","iopub.execute_input":"2022-02-05T19:31:46.543018Z","iopub.status.idle":"2022-02-05T19:31:46.558525Z","shell.execute_reply.started":"2022-02-05T19:31:46.542980Z","shell.execute_reply":"2022-02-05T19:31:46.557807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize some images","metadata":{}},{"cell_type":"code","source":"train_image_list = glob.glob(TRAIN_IMAGE_PATH + '/*.png')\ntest_image_list = glob.glob(TEST_IMAGE_PATH + '/*.png')\n\ntrain_df =pd.read_csv(TRAIN_LABEL_PATH)\n\n#Types of cells\nunique_train_cell_types = pd.unique(train_df['cell_type'])\n\nprint(\"Unique cell types: \", unique_train_cell_types, \"\\n\")\n\nfor cell_type in unique_train_cell_types:\n    num_of_occ = len(train_df[train_df['cell_type'] == cell_type])\n    print(cell_type, \": \", num_of_occ, \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-02-05T19:31:46.559556Z","iopub.execute_input":"2022-02-05T19:31:46.560295Z","iopub.status.idle":"2022-02-05T19:31:47.432064Z","shell.execute_reply.started":"2022-02-05T19:31:46.560244Z","shell.execute_reply":"2022-02-05T19:31:47.431168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Print a few images\n_, axs = plt.subplots(3,2, figsize=(20,20))\n\n\nfor cell_type_num in range(len(unique_train_cell_types)):\n    cell_type = unique_train_cell_types[cell_type_num]\n    temp_df = train_df[train_df['cell_type'] == cell_type].iloc[0]\n    image_id = temp_df[\"id\"]\n    enc = temp_df['annotation']\n    image_height = temp_df['height']\n    image_width = temp_df['width']\n    dec = rle_decode(mask_rle = enc, shape=(image_height, image_width))\n    \n    train_img_index = [i for i in range(len(train_image_list)) if  image_id in train_image_list[i]]\n    image = img.imread(train_image_list[train_img_index[0]])\n    \n    axs[cell_type_num][0].imshow(image, cmap='gray')\n    axs[cell_type_num][1].imshow(dec, cmap='gray')\n    axs[cell_type_num][1].set_title(cell_type, fontsize=50)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-05T19:31:47.433196Z","iopub.execute_input":"2022-02-05T19:31:47.433949Z","iopub.status.idle":"2022-02-05T19:31:49.345041Z","shell.execute_reply.started":"2022-02-05T19:31:47.433908Z","shell.execute_reply":"2022-02-05T19:31:49.341980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define functions to transform/augment data","metadata":{}},{"cell_type":"code","source":"class Compose:\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, image, target):\n        for t in self.transforms:\n            image, target = t(image, target)\n        return image, target\n\nclass VerticalFlip:\n    def __init__(self, prob):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if random.random() <= self.prob:\n            height, width = image.shape[-2:]\n            image = image.flip(-2)\n            bbox = target[\"boxes\"]\n            bbox[:, [1, 3]] = height - bbox[:, [3, 1]]\n            target[\"boxes\"] = bbox\n            target[\"masks\"] = target[\"masks\"].flip(-2)\n        return image, target\n\nclass HorizontalFlip:\n    def __init__(self, prob):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if random.random() <= self.prob:\n            height, width = image.shape[-2:]\n            image = image.flip(-1)\n            bbox = target[\"boxes\"]\n            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n            target[\"boxes\"] = bbox\n            target[\"masks\"] = target[\"masks\"].flip(-1)\n        return image, target\n    \nclass Rotation_2D:\n    def __init__(self, prob, rot_deg_range):\n        self.prob = prob\n        self.rot_deg_range = rot_deg_range\n    \n    def __call__(self, image, target):\n        if random.random() <= self.prob:\n            #Generate random rotation angle in given range\n            rot_angle = random.uniform(self.rot_deg_range[0], self.rot_deg_range[1])\n            \n            #Get average background pixel level, to use for filling open areas after image rotation\n            masks = target[\"masks\"]\n            comb_masks = combine_masks(masks, mask_threshold = 0)\n            comb_masks_compliment = torch.as_tensor(comb_masks != 1, dtype=torch.float32)\n            fill_value = (torch.sum(torch.sum(image*comb_masks_compliment))/torch.sum(torch.sum(comb_masks_compliment))).item()\n            \n            #Rotate image\n            image = F.rotate(img = image, angle = rot_angle, interpolation = InterpolationMode.BILINEAR, expand = False, fill = fill_value)\n            \n            #Rotate mask\n            rot_masks_temp = F.rotate(img = masks, angle = rot_angle, interpolation = InterpolationMode.NEAREST, expand = False, fill = 0)\n            \n            #Remove masks images which became empty after rotation (due to mask going out of image area) & make boxes for masks still in image area\n            boxes = []\n            rot_masks = np.zeros(rot_masks_temp.size(), dtype=np.uint8)\n            for i in range(rot_masks_temp.size(dim = 0)):\n                is_all_zero = np.all(np.array(rot_masks_temp[i, :, :] == 0))\n                if not is_all_zero:\n                    temp_box = get_box(rot_masks_temp[i, :, : ])\n                    if temp_box:\n                        rot_masks[i, :, : ] = np.array(rot_masks_temp[i, :, :])\n                        boxes.append(temp_box)\n            \n            #Assign updated values to boxes, labels, masks, iscrowd keys in target dictionary\n            n_objects = len(boxes)       \n            labels = [target[\"labels\"][0] for _ in range(n_objects)]        \n            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n            labels = torch.as_tensor(labels, dtype=torch.int64)\n            rot_masks = torch.as_tensor(rot_masks, dtype=torch.uint8) \n            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n            iscrowd = torch.zeros((n_objects,), dtype=torch.int64)\n            \n            target[\"area\"] = area\n            target[\"boxes\"] = boxes\n            target[\"masks\"] = rot_masks\n            target[\"iscrowd\"] = iscrowd\n            \n        return image, target\n                   \n\nclass Normalize:\n    def __call__(self, image, target):\n        image = F.normalize(image, RESNET_MEAN, RESNET_STD)\n        return image, target\n\nclass ToTensor:\n    def __call__(self, image, target):\n        image = F.to_tensor(image)\n        return image, target\n    \n\nclass EnhanceEdges:\n    def __init__(self):\n        None\n    def __call__(self, image, target):\n        enhanced_edge_image = image.filter(ImageFilter.EDGE_ENHANCE)\n        return enhanced_edge_image, target\n    \n\ndef get_transform(train, rot_deg_range = (-180, 180), horz_prob=0, vert_prob=0, rot_prob=0):\n    #transforms = [EnhanceEdges()]\n    transforms = [ToTensor()]\n    #transforms.append(ToTensor()) \n    if NORMALIZE:\n        transforms.append(Normalize())\n    \n    # Data augmentation for train\n    if train: \n        transforms.append(HorizontalFlip(horz_prob))\n        transforms.append(VerticalFlip(vert_prob))\n        transforms.append(Rotation_2D(rot_prob, rot_deg_range))\n\n    return Compose(transforms)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T19:31:49.346297Z","iopub.execute_input":"2022-02-05T19:31:49.346697Z","iopub.status.idle":"2022-02-05T19:31:49.374251Z","shell.execute_reply.started":"2022-02-05T19:31:49.346656Z","shell.execute_reply":"2022-02-05T19:31:49.373575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training and validation dataset","metadata":{}},{"cell_type":"code","source":"class CellDataset(Dataset):\n    def __init__(self, image_dir, df, transforms=None, resize=False):\n        self.transforms = transforms\n        self.image_dir = image_dir\n        self.df = df\n        \n        self.should_resize = resize is not False\n        if self.should_resize:\n            self.height = int(HEIGHT * resize)\n            self.width = int(WIDTH * resize)\n        else:\n            self.height = HEIGHT\n            self.width = WIDTH\n        \n        self.image_info = collections.defaultdict(dict)\n        temp_df = self.df.groupby([\"id\", \"cell_type\"])['annotation'].agg(lambda x: list(x)).reset_index()\n        for index, row in temp_df.iterrows():\n            self.image_info[index] = {\n                    'image_id': row['id'],\n                    'image_path': os.path.join(self.image_dir, row['id'] + '.png'),\n                    'annotations': row[\"annotation\"],\n                    'cell_type': cell_type_dict[row[\"cell_type\"]]\n                    }\n    \n    def __getitem__(self, idx):\n        ''' Get the image and the target'''\n        \n        img_path = self.image_info[idx][\"image_path\"]\n        img = Image.open(img_path).convert(\"RGB\")\n        #img = Image.open(img_path)\n        \n        if self.should_resize:\n            img = img.resize((self.width, self.height), resample=Image.BILINEAR)\n\n        info = self.image_info[idx]\n\n        n_objects = len(info['annotations'])\n        masks = np.zeros((len(info['annotations']), self.height, self.width), dtype=np.uint8)\n        boxes = []\n        \n        for i, annotation in enumerate(info['annotations']):\n            a_mask = rle_decode(annotation, (HEIGHT, WIDTH))\n            a_mask = Image.fromarray(a_mask)\n            \n            if self.should_resize:\n                a_mask = a_mask.resize((self.width, self.height), resample=Image.BILINEAR)\n            \n            a_mask = np.array(a_mask) > 0\n            masks[i, :, :] = a_mask\n            \n            boxes.append(get_box(a_mask))\n\n        # labels\n        labels = [int(info[\"cell_type\"]) for _ in range(n_objects)]\n        \n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        masks = torch.as_tensor(masks, dtype=torch.uint8)\n\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        iscrowd = torch.zeros((n_objects,), dtype=torch.int64)\n\n        # This is the required target for the Mask R-CNN\n        target = {\n            'boxes': boxes,\n            'labels': labels,\n            'masks': masks,\n            'image_id': image_id,\n            'area': area,\n            'iscrowd': iscrowd\n        }\n        \n        masks_list = []\n        for i in range(target['masks'].size(dim = 0)):\n            masks_list.append(np.array(target['masks'][i, :, :]))\n\n        if self.transforms is not None:\n                        \n            # Convert PIL image to numpy array\n            img_np = np.array(img)\n            \n            # Apply transformations\n            transformed = self.transforms(image=img_np, masks = masks_list)\n            \n            transformed_img = transformed[\"image\"]\n            transformed_masks = transformed[\"masks\"]\n        \n              \n            transformed_boxes = []\n            transformed_masks_list = []\n            for i in range(len(transformed_masks)):\n                tmp_box = get_box(np.array(transformed_masks[i]))\n                if tmp_box:\n                    transformed_boxes.append(tmp_box)\n                    transformed_masks_list.append(np.array(transformed_masks[i]))\n                    \n                    \n            transformed_masks = np.zeros((len(transformed_boxes), self.height, self.width), dtype=np.uint8)\n            for i in range(len(transformed_boxes)):\n                transformed_masks[i, :, :] = transformed_masks_list[i]\n            \n            iscrowd = torch.zeros((len(transformed_boxes),), dtype=torch.int64)\n            labels = [int(info[\"cell_type\"]) for _ in range(len(transformed_boxes))]\n            \n            transformed_boxes = torch.as_tensor(transformed_boxes, dtype=torch.float32)\n            transformed_masks = torch.as_tensor(transformed_masks, dtype=torch.uint8)\n            labels = torch.as_tensor(labels, dtype=torch.int64)\n            \n            if transformed_boxes.size(dim = 0) > 0:\n                transformed_area = (transformed_boxes[:, 3] - transformed_boxes[:, 1]) * (transformed_boxes[:, 2] - transformed_boxes[:, 0])\n                transformed_target = {\n                'boxes': transformed_boxes,\n                'labels': labels,\n                'masks': transformed_masks,\n                'image_id': image_id,\n                'area': transformed_area,\n                'iscrowd': iscrowd\n                }\n                target = transformed_target\n            \n            img = transformed_img\n                \n        return img, target\n                            \n\n    def __len__(self):\n        return len(self.image_info)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T19:31:49.375493Z","iopub.execute_input":"2022-02-05T19:31:49.375903Z","iopub.status.idle":"2022-02-05T19:31:49.404163Z","shell.execute_reply.started":"2022-02-05T19:31:49.375859Z","shell.execute_reply":"2022-02-05T19:31:49.403613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test Dataset and DataLoader","metadata":{}},{"cell_type":"code","source":"class CellTestDataset(Dataset):\n    def __init__(self, image_dir, transforms=None):\n        self.transforms = transforms\n        self.image_dir = image_dir\n        self.image_ids = [f[:-4]for f in os.listdir(self.image_dir)]\n    \n    def __getitem__(self, idx):\n        image_id = self.image_ids[idx]\n        image_path = os.path.join(self.image_dir, image_id + '.png')\n        image = Image.open(image_path).convert(\"RGB\")\n\n        if self.transforms is not None:\n            image, _ = self.transforms(image=image, target=None)\n        return {'image': image, 'image_id': image_id}\n\n    def __len__(self):\n        return len(self.image_ids)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T19:31:49.405792Z","iopub.execute_input":"2022-02-05T19:31:49.406320Z","iopub.status.idle":"2022-02-05T19:31:49.418038Z","shell.execute_reply.started":"2022-02-05T19:31:49.406275Z","shell.execute_reply":"2022-02-05T19:31:49.417391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_base = pd.read_csv(TRAIN_LABEL_PATH)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-05T19:31:49.420152Z","iopub.execute_input":"2022-02-05T19:31:49.420462Z","iopub.status.idle":"2022-02-05T19:31:49.770167Z","shell.execute_reply.started":"2022-02-05T19:31:49.420422Z","shell.execute_reply":"2022-02-05T19:31:49.769461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_images = df_base.groupby([\"id\", \"cell_type\"]).agg({'annotation': 'count'}).sort_values(\"annotation\", ascending=False).reset_index()\n\nfor ct in cell_type_dict:\n    ctdf = df_images[df_images[\"cell_type\"]==ct].copy()\n    if len(ctdf)>0:\n        ctdf['quantiles'] = pd.qcut(ctdf['annotation'], 5)\n        display(ctdf.head())","metadata":{"execution":{"iopub.status.busy":"2022-02-05T19:31:49.772593Z","iopub.execute_input":"2022-02-05T19:31:49.772929Z","iopub.status.idle":"2022-02-05T19:31:49.851972Z","shell.execute_reply.started":"2022-02-05T19:31:49.772897Z","shell.execute_reply":"2022-02-05T19:31:49.851394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_images.groupby(\"cell_type\").annotation.describe().astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T19:31:49.853033Z","iopub.execute_input":"2022-02-05T19:31:49.853668Z","iopub.status.idle":"2022-02-05T19:31:49.876600Z","shell.execute_reply.started":"2022-02-05T19:31:49.853633Z","shell.execute_reply":"2022-02-05T19:31:49.876062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We used this as a reference to fill BOX_DETECTIONS_PER_IMG=140\ndf_images[['annotation']].describe().astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T19:31:49.877748Z","iopub.execute_input":"2022-02-05T19:31:49.877989Z","iopub.status.idle":"2022-02-05T19:31:49.892454Z","shell.execute_reply.started":"2022-02-05T19:31:49.877962Z","shell.execute_reply":"2022-02-05T19:31:49.891510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use the quantiles of amoount of annotations to stratify\ndf_images_train, df_images_val = train_test_split(df_images, stratify=df_images['cell_type'], test_size=PCT_IMAGES_VALIDATION)\ndf_train = df_base[df_base['id'].isin(df_images_train['id'])]\ndf_val = df_base[df_base['id'].isin(df_images_val['id'])]\nprint(f\"Images in train set:           {len(df_images_train)}\")\nprint(f\"Annotations in train set:      {len(df_train)}\")\nprint(f\"Images in validation set:      {len(df_images_val)}\")\nprint(f\"Annotations in validation set: {len(df_val)}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-05T19:31:49.893699Z","iopub.execute_input":"2022-02-05T19:31:49.894129Z","iopub.status.idle":"2022-02-05T19:31:49.932268Z","shell.execute_reply.started":"2022-02-05T19:31:49.894098Z","shell.execute_reply":"2022-02-05T19:31:49.931453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_transform_album(train = True, horz_flip_prob = 0.5, vert_flip_prob = 0.5, shiftscalerotate_prob = 0.5, \\\n                        shift_limit_factor = 0.05, scale_limit_factor = 0.1, rotate_limit_deg = 45, \\\n                        distort_limit = 0.05, distort_prob = 0.5):\n    train_transform_album = A.Compose([A.Normalize(mean=RESNET_MEAN, std=RESNET_STD), \\\n                                      A.HorizontalFlip(p=horz_flip_prob), A.VerticalFlip(p=vert_flip_prob), \\\n                                      A.ShiftScaleRotate(shift_limit=shift_limit_factor, scale_limit=scale_limit_factor, rotate_limit=rotate_limit_deg, p=shiftscalerotate_prob, \\\n                                                         border_mode = 1, value = 0, mask_value = 0), \\\n                                      A.OpticalDistortion(distort_limit=distort_limit, shift_limit=0, p = distort_prob), \\\n                                      ToTensorV2()])\n    validation_transform_album = A.Compose([A.Normalize(mean=RESNET_MEAN, std=RESNET_STD), ToTensorV2()])\n    \n    if train:\n        return train_transform_album\n    else:\n        return validation_transform_album","metadata":{"execution":{"iopub.status.busy":"2022-02-05T19:31:49.933495Z","iopub.execute_input":"2022-02-05T19:31:49.934017Z","iopub.status.idle":"2022-02-05T19:31:49.941545Z","shell.execute_reply.started":"2022-02-05T19:31:49.933979Z","shell.execute_reply":"2022-02-05T19:31:49.940969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_train_orig_horz_vert_flip = CellDataset(TRAIN_IMAGE_PATH, df_train, resize=resize_factor, \\\n                            transforms = get_transform_album(train = True, horz_flip_prob = 0.5, vert_flip_prob = 0.5, shiftscalerotate_prob = 0, \\\n                            shift_limit_factor = 0.05, scale_limit_factor = 0.1, rotate_limit_deg = 45, \\\n                            distort_limit = 0.05, distort_prob = 0))\n                            #transforms=get_transform(train=True, rot_deg_range = (-180, 180), horz_prob=0.5, vert_prob=0.5, rot_prob=0))\n\ndf_train_50_perc1 = df_train.sample(frac = 0.5)\ndf_train_50_perc2 = df_train.drop(df_train_50_perc1.index).reset_index()\ndf_train_50_perc1 = df_train_50_perc1.reset_index()\n\nds_train_shiftscalerot = CellDataset(TRAIN_IMAGE_PATH, df_train_50_perc1, resize=resize_factor, \\\n                           transforms = get_transform_album(train = True, horz_flip_prob = 0, vert_flip_prob = 0, shiftscalerotate_prob = 1, \\\n                           shift_limit_factor = 0.1, scale_limit_factor = 0.1, rotate_limit_deg = 180, \\\n                           distort_limit = 0.05, distort_prob = 0)) #Keep the shift_limit_factor small enough\n                           #transforms=get_transform(train=True, rot_deg_range = (-180, 180), horz_prob=0, vert_prob=0, rot_prob=1))\n        \nds_train_distort = CellDataset(TRAIN_IMAGE_PATH, df_train_50_perc2, resize=resize_factor, \\\n                           transforms = get_transform_album(train = True, horz_flip_prob = 0, vert_flip_prob = 0, shiftscalerotate_prob = 0, \\\n                           shift_limit_factor = 0.5, scale_limit_factor = 0.1, rotate_limit_deg = 180, \\\n                           distort_limit = 0.05, distort_prob = 1)) \n\n\nds_train = ConcatDataset([ds_train_orig_horz_vert_flip, ds_train_shiftscalerot, ds_train_distort])\n#ds_train = ds_train_distort\n\n\ndl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,\n                      num_workers=2, collate_fn=lambda x: tuple(zip(*x)))\n\nds_val = CellDataset(TRAIN_IMAGE_PATH, df_val, resize=resize_factor, transforms=get_transform_album(train=False))\ndl_val = DataLoader(ds_val, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,\n                    num_workers=2, collate_fn=lambda x: tuple(zip(*x)))","metadata":{"execution":{"iopub.status.busy":"2022-02-05T19:31:49.942530Z","iopub.execute_input":"2022-02-05T19:31:49.942853Z","iopub.status.idle":"2022-02-05T19:31:50.208319Z","shell.execute_reply.started":"2022-02-05T19:31:49.942826Z","shell.execute_reply":"2022-02-05T19:31:50.207751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_test = CellTestDataset(TEST_IMAGE_PATH, transforms=get_transform(train=False))","metadata":{"execution":{"iopub.status.busy":"2022-02-05T19:31:50.209660Z","iopub.execute_input":"2022-02-05T19:31:50.210168Z","iopub.status.idle":"2022-02-05T19:31:50.215613Z","shell.execute_reply.started":"2022-02-05T19:31:50.210127Z","shell.execute_reply":"2022-02-05T19:31:50.214676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(ds_train))","metadata":{"execution":{"iopub.status.busy":"2022-02-05T19:32:16.629898Z","iopub.execute_input":"2022-02-05T19:32:16.630195Z","iopub.status.idle":"2022-02-05T19:32:16.635029Z","shell.execute_reply.started":"2022-02-05T19:32:16.630162Z","shell.execute_reply":"2022-02-05T19:32:16.634273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LIVE CELL DATASET","metadata":{}},{"cell_type":"code","source":"if USE_LIVECELL:\n    \n    LIVECELL_DATASET_TRAIN_PATH = '../input/sartorius-cell-instance-segmentation/LIVECell_dataset_2021/annotations/LIVECell/livecell_coco_train.json'\n    LIVECELL_DATASET_TEST_PATH = '../input/sartorius-cell-instance-segmentation/LIVECell_dataset_2021/annotations/LIVECell/livecell_coco_test.json'\n    LIVECELL_DATASET_VAL_PATH = '../input/sartorius-cell-instance-segmentation/LIVECell_dataset_2021/annotations/LIVECell/livecell_coco_val.json'\n\n    with open(LIVECELL_DATASET_TRAIN_PATH) as f:\n        livecell_dataset_train_annot = json.load(f)\n\n    with open(LIVECELL_DATASET_TEST_PATH) as f:\n        livecell_dataset_test_annot = json.load(f)\n\n    with open(LIVECELL_DATASET_VAL_PATH) as f:\n        livecell_dataset_val_annot = json.load(f)\n\n    livecell_dataset_train_dict = convert_livecell_annot_to_dict(livecell_dataset_train_annot)\n    livecell_dataset_test_dict = convert_livecell_annot_to_dict(livecell_dataset_test_annot)\n    livecell_dataset_val_dict = convert_livecell_annot_to_dict(livecell_dataset_val_annot)\n    \n    ## Convert LIVE CELL DATASET to dataframe of the same format as df_base\n    df_livecell_train_df = pd.DataFrame(columns = ['id', 'annotation', 'width', 'height', 'cell_type', 'sample_id'])\n\n    for key in livecell_dataset_train_dict.keys():\n        image_name = livecell_dataset_train_dict[key]['path'][0][:-4]\n        annotation_list = convert_livecell_mask_to_rle(livecell_dataset_train_dict[key]['segmentation']) \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-01-04T15:51:56.093627Z","iopub.execute_input":"2022-01-04T15:51:56.094196Z","iopub.status.idle":"2022-01-04T15:51:56.10422Z","shell.execute_reply.started":"2022-01-04T15:51:56.094151Z","shell.execute_reply":"2022-01-04T15:51:56.103585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train loop","metadata":{}},{"cell_type":"markdown","source":"## Mask RCNN Model","metadata":{}},{"cell_type":"code","source":"if USE_MASK_RCNN:\n    \n    # Override pythorch checkpoint with an \"offline\" version of the pretrained MaskRCNN model file\n    !mkdir -p /root/.cache/torch/hub/checkpoints/\n    !cp ../input/maskrcnn-pretrained/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth","metadata":{"execution":{"iopub.status.busy":"2022-01-04T15:51:56.10547Z","iopub.execute_input":"2022-01-04T15:51:56.105879Z","iopub.status.idle":"2022-01-04T15:52:01.463708Z","shell.execute_reply.started":"2022-01-04T15:51:56.105848Z","shell.execute_reply":"2022-01-04T15:52:01.462723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if USE_MASK_RCNN:\n    \n    def get_model(num_classes, model_chkpt=None):\n        # This is just a dummy value for the classification head\n\n        if NORMALIZE:\n            model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True,\n                                                                       box_detections_per_img=BOX_DETECTIONS_PER_IMG,\n                                                                       image_mean=RESNET_MEAN,\n                                                                       image_std=RESNET_STD)\n        else:\n            model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True,\n                                                                       box_detections_per_img=BOX_DETECTIONS_PER_IMG)\n\n        # get the number of input features for the classifier\n        in_features = model.roi_heads.box_predictor.cls_score.in_features\n        # replace the pre-trained head with a new one\n        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes+1)\n\n        # now get the number of input features for the mask classifier\n        in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n        hidden_layer = 256\n        # and replace the mask predictor with a new one\n        model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes+1)\n\n        if model_chkpt:\n            model.load_state_dict(torch.load(model_chkpt, map_location=DEVICE))\n        return model\n\n    # Get the Mask R-CNN model\n    # The model does classification, bounding boxes and MASKs for individuals, all at the same time\n    # We only care about MASKS\n    model = get_model(len(cell_type_dict))\n    model.to(DEVICE)\n\n    # TODO: try removing this for\n    for param in model.parameters():\n        param.requires_grad = True\n\n    model.train();","metadata":{"execution":{"iopub.status.busy":"2022-01-04T15:52:01.465305Z","iopub.execute_input":"2022-01-04T15:52:01.465638Z","iopub.status.idle":"2022-01-04T15:52:02.506246Z","shell.execute_reply.started":"2022-01-04T15:52:01.465587Z","shell.execute_reply":"2022-01-04T15:52:02.505554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"child_counter = 0\nfor child in model.children():\n    print(\" child\", child_counter, \"is -\")\n    print(child)\n    child_counter += 1","metadata":{"execution":{"iopub.status.busy":"2022-01-04T15:52:52.053276Z","iopub.execute_input":"2022-01-04T15:52:52.053966Z","iopub.status.idle":"2022-01-04T15:52:52.062731Z","shell.execute_reply.started":"2022-01-04T15:52:52.053931Z","shell.execute_reply":"2022-01-04T15:52:52.062097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TEST:\n    model.eval()\n    print(model)\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-19T14:38:32.809765Z","iopub.execute_input":"2021-12-19T14:38:32.810089Z","iopub.status.idle":"2021-12-19T14:38:32.819913Z","shell.execute_reply.started":"2021-12-19T14:38:32.810059Z","shell.execute_reply":"2021-12-19T14:38:32.818617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Mask RCNN Training loop!","metadata":{}},{"cell_type":"code","source":"if USE_MASK_RCNN:\n\n    model_file_name_list = []\n\n    if not TEST:\n\n        params = [p for p in model.parameters() if p.requires_grad]\n        optimizer = torch.optim.SGD(params, lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n        #optimizer = torch.optim.Adam(params, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n\n        n_batches, n_batches_val = len(dl_train), len(dl_val)\n\n        validation_mask_losses = []\n\n        for epoch in range(1, NUM_EPOCHS + 1):\n            print(f\"Starting epoch {epoch} of {NUM_EPOCHS}\")\n\n            time_start = time.time()\n            loss_accum = 0.0\n            loss_mask_accum = 0.0\n            loss_classifier_accum = 0.0\n            for batch_idx, (images, targets) in enumerate(dl_train, 1):\n\n                # Predict\n                images = list(image.to(DEVICE) for image in images)\n                targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n\n                loss_dict = model(images, targets)\n                loss = sum(loss for loss in loss_dict.values())\n\n                # Backprop\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                # Logging\n                loss_mask = loss_dict['loss_mask'].item()\n                loss_accum += loss.item()\n                loss_mask_accum += loss_mask\n                loss_classifier_accum += loss_dict['loss_classifier'].item()\n\n                if batch_idx % 100 == 0:\n                    print(f\"    [Batch {batch_idx:3d} / {n_batches:3d}] Batch train loss: {loss.item():7.3f}. Mask-only loss: {loss_mask:7.3f}.\")\n\n            if USE_SCHEDULER:\n                lr_scheduler.step()\n\n            # Train losses\n            train_loss = loss_accum / n_batches\n            train_loss_mask = loss_mask_accum / n_batches\n            train_loss_classifier = loss_classifier_accum / n_batches\n\n            # Validation\n            val_loss_accum = 0\n            val_loss_mask_accum = 0\n            val_loss_classifier_accum = 0\n\n            with torch.no_grad():\n                for batch_idx, (images, targets) in enumerate(dl_val, 1):\n                    images = list(image.to(DEVICE) for image in images)\n                    targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n\n                    val_loss_dict = model(images, targets)\n                    val_batch_loss = sum(loss for loss in val_loss_dict.values())\n                    val_loss_accum += val_batch_loss.item()\n                    val_loss_mask_accum += val_loss_dict['loss_mask'].item()\n                    val_loss_classifier_accum += val_loss_dict['loss_classifier'].item()\n\n            # Validation losses\n            val_loss = val_loss_accum / n_batches_val\n            val_loss_mask = val_loss_mask_accum / n_batches_val\n            val_loss_classifier = val_loss_classifier_accum / n_batches_val\n            elapsed = time.time() - time_start\n\n            validation_mask_losses.append(val_loss_mask)\n\n            torch.save(model.state_dict(), f\"pytorch_model-e{epoch}.bin\")\n            model_file_name_list.append(f\"pytorch_model-e{epoch}.bin\")\n            prefix = f\"[Epoch {epoch:2d} / {NUM_EPOCHS:2d}]\"\n            print(prefix)\n            print(f\"{prefix} Train mask-only loss: {train_loss_mask:7.3f}, classifier loss {train_loss_classifier:7.3f}\")\n            print(f\"{prefix} Val mask-only loss  : {val_loss_mask:7.3f}, classifier loss {val_loss_classifier:7.3f}\")\n            print(prefix)\n            print(f\"{prefix} Train loss: {train_loss:7.3f}. Val loss: {val_loss:7.3f} [{elapsed:.0f} secs]\")\n            print(prefix)\n\n    else:\n\n        model_file_name_list = glob.glob('../input/maskrcnn-finetuned' + '/*.bin')     \n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-19T14:24:52.301923Z","iopub.execute_input":"2021-12-19T14:24:52.302262Z","iopub.status.idle":"2021-12-19T14:24:52.335267Z","shell.execute_reply.started":"2021-12-19T14:24:52.302218Z","shell.execute_reply":"2021-12-19T14:24:52.334391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Mask RCNN: Analyze prediction results for train set","metadata":{}},{"cell_type":"code","source":"# Plots: the image, The image + the ground truth mask, The image + the predicted mask\n\ndef analyze_train_sample(model, ds_train, sample_index):\n    \n    img, targets = ds_train[sample_index]\n    #print(img.shape)\n    l = np.unique(targets[\"labels\"])\n    ig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20,60), facecolor=\"#fefefe\")\n    ax[0].imshow(img.numpy().transpose((1,2,0)), cmap = \"gray\")\n    #ax[0].set_title(f\"cell type {l}\")\n    ax[0].set_title(f\"cell type: {get_key(cell_type_dict, l)}\", )\n    ax[0].axis(\"off\")\n    \n    masks = combine_masks(targets['masks'], 0)\n    #plt.imshow(img.numpy().transpose((1,2,0)))\n    ax[1].imshow(masks, cmap = \"gray\")\n    ax[1].set_title(f\"Ground truth, {len(targets['masks'])} cells\")\n    ax[1].axis(\"off\")\n    \n    model.eval()\n    with torch.no_grad():\n        preds = model([img.to(DEVICE)])[0]\n    \n    l = pd.Series(preds['labels'].cpu().numpy()).value_counts()\n    print(l)\n    lstr = \"\"\n    for i in l.index:\n        lstr += f\"{l[i]}x{i} \"\n    #print(l, l.sort_values().index[-1])\n    #plt.imshow(img.cpu().numpy().transpose((1,2,0)))\n    mask_threshold = mask_threshold_dict[l.sort_values().index[-1]]\n    #print(mask_threshold)\n    pred_masks = combine_masks(get_filtered_masks(preds), mask_threshold)\n    ax[2].imshow(pred_masks, cmap = \"gray\")\n    ax[2].set_title(f\"Predictions, labels: {lstr}\")\n    ax[2].axis(\"off\")\n    plt.show() \n        \n    #print(masks.shape, pred_masks.shape)\n    score = iou_map([masks],[pred_masks])\n    print(\"Score:\", score)    \n    \n    \n# NOTE: It puts the model in eval mode!! Revert for re-training\n#analyze_train_sample(model, ds_train, 20)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-19T14:24:52.338447Z","iopub.execute_input":"2021-12-19T14:24:52.339664Z","iopub.status.idle":"2021-12-19T14:24:52.353179Z","shell.execute_reply.started":"2021-12-19T14:24:52.339568Z","shell.execute_reply":"2021-12-19T14:24:52.352398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Mask RCNN: Get the model from the best epoch","metadata":{}},{"cell_type":"code","source":"if USE_MASK_RCNN:\n    if TEST and BEST_EPOCH is None:\n\n        # Epochs with their losses and IOU scores\n        val_scores = pd.DataFrame()\n        for model_file_name in model_file_name_list:\n            #model_chk = f\"pytorch_model-e{e+1}.bin\"\n            #print(\"Loading:\", model_chk)\n            #model = get_model(len(cell_type_dict), model_chk)\n            #model.load_state_dict(torch.load(model_chk))\n            #model = model.to(DEVICE)\n            #val_scores.loc[e,\"mask_loss\"] = val_loss\n            model_state_dict = torch.load(model_file_name, map_location=DEVICE)\n            model.load_state_dict(model_state_dict)\n\n            val_scores.loc[int(model_file_name.split('-e')[1].split('.bin')[0])-1,\"score\"], _ = get_score(ds_val, model)\n\n        val_scores.sort_index(axis = 0, inplace = True)\n        display(val_scores.sort_values(\"score\", ascending=False, inplace = False))\n\n        best_epoch = val_scores[\"score\"].idxmax()\n        BEST_EPOCH = best_epoch+1\n\n\n    print(\"Best Epoch: \", BEST_EPOCH)\n    best_model_file_name = [model_file_name for model_file_name in model_file_name_list if str(BEST_EPOCH) in model_file_name]\n    model_state_dict = torch.load(best_model_file_name[0], map_location=DEVICE)\n    model.load_state_dict(model_state_dict)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T14:24:52.354324Z","iopub.execute_input":"2021-12-19T14:24:52.35534Z","iopub.status.idle":"2021-12-19T14:24:56.189841Z","shell.execute_reply.started":"2021-12-19T14:24:52.355296Z","shell.execute_reply":"2021-12-19T14:24:56.189095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Mask RCNN: Checking Validation Images with low scores, use model with best epoch","metadata":{}},{"cell_type":"code","source":"if USE_MASK_RCNN:\n    if TEST:\n        prob_image_ind_filename = '../input/problem-images-dataset-indices/problem_images_dataset_indices.pkl'\n        if os.path.isfile(prob_image_ind_filename):\n            with open(prob_image_ind_filename, 'rb') as f:\n                problem_images = pickle.load(f)\n        else:\n            avg_score, score_list = get_score(ds_val, model)\n            problem_images = np.where(np.array(score_list) < 0.2)\n            with open('./problem_images_dataset_indices.pkl', 'wb') as f:\n                pickle.dump(problem_images, f)\n\n        _, axs = plt.subplots(1,2, figsize=(20,20))\n        image_id = ds_val.image_info[problem_images[0][3]]['image_id']\n        train_img_index = [i for i in range(len(train_image_list)) if  image_id in train_image_list[i]]\n        image = Image.open(train_image_list[train_img_index[0]]) \n        #image = img.imread(train_image_list[train_img_index[0]])\n        enhanced_image  = image.filter(ImageFilter.EDGE_ENHANCE)\n\n        axs[0].imshow(image, cmap = 'gray')\n        axs[1].imshow(enhanced_image, cmap = 'gray')\n","metadata":{"execution":{"iopub.status.busy":"2021-12-19T14:24:56.191184Z","iopub.execute_input":"2021-12-19T14:24:56.19161Z","iopub.status.idle":"2021-12-19T14:25:38.896156Z","shell.execute_reply.started":"2021-12-19T14:24:56.191576Z","shell.execute_reply":"2021-12-19T14:25:38.894845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Mask RCNN: Prediction","metadata":{}},{"cell_type":"code","source":"if USE_MASK_RCNN:\n    if TEST:\n\n        model.eval();\n\n        submission = []\n        for sample in ds_test:\n            img = sample['image']\n            image_id = sample['image_id']\n            with torch.no_grad():\n                result = model([img.to(DEVICE)])[0]\n\n            previous_masks = []\n            for i, mask in enumerate(result[\"masks\"]):\n\n                # Filter-out low-scoring results. Not tried yet.\n                score = result[\"scores\"][i].cpu().item()\n                label = result[\"labels\"][i].cpu().item()\n                if score > min_score_dict[label]:\n                    mask = mask.cpu().numpy()\n                    # Keep only highly likely pixels\n                    binary_mask = mask > mask_threshold_dict[label]\n                    binary_mask = remove_overlapping_pixels(binary_mask, previous_masks)\n                    previous_masks.append(binary_mask)\n                    rle = rle_encode(binary_mask)\n                    submission.append((image_id, rle))\n\n\n\n\n            # Add empty prediction if no RLE was generated for this image\n            all_images_ids = [image_id for image_id, rle in submission]\n            if image_id not in all_images_ids:\n                submission.append((image_id, \"\"))\n\n        df_sub = pd.DataFrame(submission, columns=['id', 'predicted'])\n        df_sub.to_csv(\"submission.csv\", index=False)\n        df_sub.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-19T14:25:38.897119Z","iopub.status.idle":"2021-12-19T14:25:38.89746Z","shell.execute_reply.started":"2021-12-19T14:25:38.897281Z","shell.execute_reply":"2021-12-19T14:25:38.897302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}